# Practica8_Monitorizacion_Completa.md

# üìò Pr√°c¬≠tica 8 ‚Äî Moni¬≠tori¬≠za¬≠ci√≥n y Lo¬≠gging en Kubernetes (Prometheus, Grafana y ELK)
**Curso:** Software Avanzado (SA)  
**Estudiante:** [Tu nombre]  
**Secci√≥n:** B  
**Fecha:** Octubre 2025

---

## √çndice
1. Resumen y objetivos
2. Requisitos previos
3. Estructura del repositorio y archivos incluidos
4. Creaci√≥n del cl√∫ster (script)
5. Namespaces y manifiestos de los microservicios (`user` y `content`)
6. Instalaci√≥n de Prometheus (Helm) + reglas de alerta
7. Instalaci√≥n de Grafana + importar dashboard
8. Instalaci√≥n de ELK (Elasticsearch, Kibana) y Filebeat
9. Conexiones y verificaci√≥n
10. Pruebas y generaci√≥n de carga
11. Entrega y notas finales
12. Anexos: todos los archivos (YAML/JSON/SH)

---

## 1. Resumen y objetivos
Implementar y configurar soluciones avanzadas de monitorizaci√≥n y logging utilizando Prometheus, Grafana y ELK sobre un cl√∫ster GKE. Centralizar logs de los microservicios `user` y `content`, recolectar m√©tricas, crear dashboards y configurar alertas.

---

## 2. Requisitos previos
- Cuenta de Google Cloud con facturaci√≥n habilitada.
- `gcloud` CLI instalado y autenticado.
- `kubectl` instalado.
- `helm` instalado.
- Acceso al cl√∫ster GKE (`gcloud container clusters get-credentials ...`).
- Repositorio con la estructura sugerida (se indica m√°s adelante).

---

## 3. Estructura del repositorio y archivos incluidos

```
Practica8/
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îî‚îÄ‚îÄ Practica8_Monitorizacion_Completa.md   <- este archivo
‚îú‚îÄ‚îÄ infra/
‚îÇ   ‚îú‚îÄ‚îÄ create-cluster.sh
‚îÇ   ‚îú‚îÄ‚îÄ prometheus/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ alert-rules.yaml
‚îÇ   ‚îú‚îÄ‚îÄ grafana/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chapinflix-dashboard.json
‚îÇ   ‚îî‚îÄ‚îÄ elk/
‚îÇ       ‚îú‚îÄ‚îÄ elasticsearch-values.yaml
‚îÇ       ‚îú‚îÄ‚îÄ kibana-values.yaml
‚îÇ       ‚îî‚îÄ‚îÄ filebeat-values.yaml
‚îî‚îÄ‚îÄ manifests/
    ‚îú‚îÄ‚îÄ namespace.yaml
    ‚îú‚îÄ‚îÄ user-deploy.yaml
    ‚îú‚îÄ‚îÄ content-deploy.yaml
    ‚îú‚îÄ‚îÄ user-service.yaml
    ‚îî‚îÄ‚îÄ content-service.yaml
```

> En la secci√≥n Anexos (final) encontrar√°s el contenido completo de cada archivo listo para copiar/pegar.

---

## 4. Creaci√≥n del cl√∫ster (script)

Archivo: `infra/create-cluster.sh`

```bash
#!/usr/bin/env bash
# usage: ./create-cluster.sh PROJECT_ID CLUSTER_NAME REGION
set -euo pipefail

PROJECT_ID="${1:-my-gcp-project}"
CLUSTER_NAME="${2:-cluster-monitor}"
REGION="${3:-us-central1}"

gcloud config set project "$PROJECT_ID"
gcloud services enable container.googleapis.com compute.googleapis.com monitoring.googleapis.com logging.googleapis.com

echo "Creando cl√∫ster GKE (Standard)..."
gcloud container clusters create "$CLUSTER_NAME" \
  --region="$REGION" \
  --machine-type=e2-standard-2 \
  --num-nodes=3 \
  --enable-ip-alias \
  --enable-autoupgrade \
  --enable-autorepair \
  --workload-pool="${PROJECT_ID}.svc.id.goog"

echo "Obteniendo credenciales..."
gcloud container clusters get-credentials "$CLUSTER_NAME" --region="$REGION"

echo "Cl√∫ster creado y credenciales configuradas."
kubectl get nodes
```

**Comando para ejecutar:**
```bash
chmod +x infra/create-cluster.sh
./infra/create-cluster.sh my-gcp-project cluster-monitor us-central1
```

---

## 5. Namespaces y manifiestos de los microservicios

### 5.1 Namespace
Archivo: `manifests/namespace.yaml`

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: chapinflix
```

Aplicar:
```bash
kubectl apply -f manifests/namespace.yaml
```

### 5.2 Microservicio `user` (Deployment + Service)
Archivo: `manifests/user-deploy.yaml`

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: user-deploy
  namespace: chapinflix
spec:
  replicas: 2
  selector:
    matchLabels:
      app: user
  template:
    metadata:
      labels:
        app: user
    spec:
      containers:
        - name: user
          image: your-registry/user:latest
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8080
          resources:
            requests:
              cpu: "100m"
              memory: "128Mi"
            limits:
              cpu: "500m"
              memory: "512Mi"
          readinessProbe:
            httpGet:
              path: /
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 10
          livenessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 10
            periodSeconds: 20
```

Archivo: `manifests/user-service.yaml`

```yaml
apiVersion: v1
kind: Service
metadata:
  name: user-service
  namespace: chapinflix
spec:
  selector:
    app: user
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080
  type: ClusterIP
```

Aplicar:
```bash
kubectl apply -f manifests/user-deploy.yaml
kubectl apply -f manifests/user-service.yaml
```

### 5.3 Microservicio `content` (Deployment + Service)
Archivo: `manifests/content-deploy.yaml`

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: content-deploy
  namespace: chapinflix
spec:
  replicas: 2
  selector:
    matchLabels:
      app: content
  template:
    metadata:
      labels:
        app: content
    spec:
      containers:
        - name: content
          image: your-registry/content:latest
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8081
          resources:
            requests:
              cpu: "100m"
              memory: "128Mi"
            limits:
              cpu: "500m"
              memory: "512Mi"
          readinessProbe:
            httpGet:
              path: /
              port: 8081
            initialDelaySeconds: 5
            periodSeconds: 10
          livenessProbe:
            httpGet:
              path: /health
              port: 8081
            initialDelaySeconds: 10
            periodSeconds: 20
```

Archivo: `manifests/content-service.yaml`

```yaml
apiVersion: v1
kind: Service
metadata:
  name: content-service
  namespace: chapinflix
spec:
  selector:
    app: content
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8081
  type: ClusterIP
```

Aplicar:
```bash
kubectl apply -f manifests/content-deploy.yaml
kubectl apply -f manifests/content-service.yaml
```

---

## 6. Instalaci√≥n de Prometheus (Helm) y reglas de alerta

### 6.1 A√±adir repositorio y crear namespace
```bash
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
kubectl create namespace monitoring
```

### 6.2 Instalar kube-prometheus-stack (Prometheus + Alertmanager + Grafana + node-exporter)
```bash
helm install prometheus prometheus-community/kube-prometheus-stack --namespace monitoring
```

Verificar:
```bash
kubectl get pods -n monitoring
kubectl get svc -n monitoring
```

### 6.3 Reglas de alerta
Archivo: `infra/prometheus/alert-rules.yaml`

```yaml
groups:
  - name: ChapinflixAlerts
    rules:
      - alert: CPUAlta
        expr: sum(rate(container_cpu_usage_seconds_total{namespace="chapinflix"}[5m])) by (pod) > 0.8
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Alto uso de CPU en pod {{ $labels.pod }}"
          description: "Uso CPU > 80% durante 2 minutos"
      - alert: MemoriaAlta
        expr: sum(container_memory_usage_bytes{namespace="chapinflix"}) by (pod) / sum(container_spec_memory_limit_bytes{namespace="chapinflix"}) by (pod) > 0.85
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Alto uso de memoria en pod {{ $labels.pod }}"
          description: "Uso de memoria > 85% durante 2 minutos"
      - alert: ReiniciosExcesivos
        expr: increase(kube_pod_container_status_restarts_total{namespace="chapinflix"}[10m]) > 2
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Reinicio excesivo de pod {{ $labels.pod }}"
          description: "M√°s de 2 reinicios en 10 minutos"
```

Aplicar reglas a Prometheus:
```bash
kubectl create configmap prometheus-chapinflix-rules --from-file=infra/prometheus/alert-rules.yaml -n monitoring || kubectl replace configmap prometheus-chapinflix-rules --from-file=infra/prometheus/alert-rules.yaml -n monitoring
# Para que Prometheus recoja el configmap, depende de la configuraci√≥n del chart; normalmente se a√±ade como additionalRuleMounts o similar.
# Reiniciar operador para cargar nuevos configmaps (si aplica)
kubectl rollout restart deployment prometheus-kube-prometheus-stack-operator -n monitoring || true
```

> Nota: dependiendo de la versi√≥n del chart, la manera de inyectar reglas puede variar. Si usaste `values.yaml` para instalar, coloca el bloque `additionalPrometheusRules` o `prometheus.prometheusSpec.ruleSelector`.

---

## 7. Instalaci√≥n de Grafana (independiente) y dashboard

### 7.1 A√±adir repo y desplegar Grafana
```bash
helm repo add grafana https://grafana.github.io/helm-charts
helm repo update
helm install grafana grafana/grafana \
  --namespace monitoring \
  --set service.type=LoadBalancer \
  --set adminUser=admin \
  --set adminPassword=admin123
```

Obtener IP:
```bash
kubectl get svc -n monitoring
```

### 7.2 Conectar Grafana con Prometheus (UI)
- URL Prometheus interno:
  ```
  http://prometheus-kube-prometheus-stack-prometheus.monitoring.svc.cluster.local:9090
  ```
- En Grafana: **Configuration ‚Üí Data Sources ‚Üí Add ‚Üí Prometheus** ‚Üí pegar la URL ‚Üí Save & Test

### 7.3 Dashboard JSON
Archivo: `infra/grafana/chapinflix-dashboard.json`

```json
{
  "dashboard": {
    "title": "Chapinflix Overview",
    "panels": [
      {
        "type": "timeseries",
        "title": "Uso CPU por Pod",
        "targets": [
          {"expr": "sum(rate(container_cpu_usage_seconds_total{namespace='chapinflix'}[5m])) by (pod)"}
        ]
      },
      {
        "type": "timeseries",
        "title": "Uso Memoria por Pod",
        "targets": [
          {"expr": "sum(container_memory_usage_bytes{namespace='chapinflix'}) by (pod)"}
        ]
      },
      {
        "type": "timeseries",
        "title": "Tr√°fico de Red RX",
        "targets": [
          {"expr": "sum(rate(container_network_receive_bytes_total{namespace='chapinflix'}[5m])) by (pod)"}
        ]
      }
    ]
  }
}
```

Importar en Grafana: **Dashboards ‚Üí Import ‚Üí Upload JSON**.

---

## 8. Instalaci√≥n del stack ELK (Elasticsearch, Kibana) y Filebeat

> Para la pr√°ctica se usa configuraci√≥n de ejemplo y un √∫nico nodo Elasticsearch. En producci√≥n se requieren varios nodos y vol√∫menes persistentes.

### 8.1 A√±adir repo Elastic y crear namespace
```bash
helm repo add elastic https://helm.elastic.co
helm repo update
kubectl create namespace elk
```

### 8.2 Instalar Elasticsearch
Archivo de valores: `infra/elk/elasticsearch-values.yaml`

```yaml
replicas: 1
esJavaOpts: "-Xmx512m -Xms512m"
resources:
  requests:
    cpu: "200m"
    memory: "1Gi"
persistence:
  enabled: false
```

Instalaci√≥n:
```bash
helm install elasticsearch elastic/elasticsearch --namespace elk -f infra/elk/elasticsearch-values.yaml
```

### 8.3 Instalar Kibana
Archivo de valores: `infra/elk/kibana-values.yaml`

```yaml
service:
  type: LoadBalancer
elasticsearchHosts: "http://elasticsearch-master:9200"
```

Instalaci√≥n:
```bash
helm install kibana elastic/kibana --namespace elk -f infra/elk/kibana-values.yaml
```

Obtener IP:
```bash
kubectl get svc -n elk
```

Acceder a Kibana: `http://<EXTERNAL-IP>:5601`

### 8.4 Instalar Filebeat (DaemonSet)
Archivo de valores: `infra/elk/filebeat-values.yaml`

```yaml
filebeatConfig:
  filebeat.yml: |
    filebeat.autodiscover:
      providers:
        - type: kubernetes
          hints.enabled: true
    processors:
      - add_cloud_metadata: {}
      - add_kubernetes_metadata: {}
    output.elasticsearch:
      hosts: ["http://elasticsearch-master:9200"]
```

Instalaci√≥n:
```bash
helm install filebeat elastic/filebeat --namespace elk -f infra/elk/filebeat-values.yaml
kubectl get pods -n elk
```

### 8.5 Configurar patr√≥n de √≠ndice en Kibana
1. En Kibana ‚Üí **Stack Management ‚Üí Index Patterns**
2. Crear patr√≥n `filebeat-*` y seleccionar `@timestamp`.

---

## 9. Conexiones y verificaciones

### 9.1 Verificar servicios
```bash
kubectl get pods -A
kubectl get svc -A
kubectl get nodes
```

### 9.2 Verificar logs en Kibana
- Acceder a Kibana ‚Üí Discover ‚Üí seleccionar `filebeat-*` ‚Üí filtrar por `namespace: "chapinflix"` o `kubernetes.pod.name: "user-deploy-..."`

### 9.3 Verificar m√©tricas en Prometheus
- Acceder al servicio Prometheus (port-forward si no est√° expuesto):
```bash
kubectl port-forward svc/prometheus-kube-prometheus-stack-prometheus 9090:9090 -n monitoring
# luego abrir http://localhost:9090
```
- Ejecutar una query de ejemplo:
```
sum(rate(container_cpu_usage_seconds_total{namespace="chapinflix"}[5m])) by (pod)
```

### 9.4 Verificar Grafana y dashboards
- Port-forward Grafana (si no tiene LoadBalancer):
```bash
kubectl port-forward svc/grafana 3000:80 -n monitoring
# abrir http://localhost:3000
```
- Importar el JSON del dashboard e inspeccionar paneles.

---

## 10. Pruebas y generaci√≥n de carga

### 10.1 Usar `kubectl run` para carga b√°sica (cURL loop)
```bash
kubectl run -i --tty load-generator --image=busybox --rm --restart=Never -- /bin/sh -c "while true; do wget -q -O- http://user-service.chapinflix.svc.cluster.local/ ; sleep 0.5; done"
```

### 10.2 Usar `hey` o `wrk` en Cloud Shell (ejemplo con `hey`)
```bash
# Instalar hey en tu m√°quina local o Cloud Shell
# en Linux:
wget https://hey-release.s3.us-east-2.amazonaws.com/hey-linux-amd64
chmod +x hey-linux-amd64
./hey-linux-amd64 -n 10000 -c 50 http://<grafana-or-service-ip>/
```

Mientras generas carga, observa m√©tricas en Grafana y registros en Kibana.

---

## 11. Entrega y notas finales
- Incluye en el repositorio `Practica8/` todos los archivos `.yaml`, `.json` y `create-cluster.sh`.
- Capturas de pantalla: `kubectl get pods -A`, dashboard Grafana, discover en Kibana, alerta disparada.
- Documenta las decisiones (por qu√© 1 r√©plica ES, por qu√© nodepool separado, etc.)
- Para producci√≥n: usar PVCs, m√∫ltiples r√©plicas de Elasticsearch, autenticaci√≥n en Kibana/Grafana, TLS y reglas de NetworkPolicy.

---

## 12. Anexos: Archivos completos

### infra/create-cluster.sh
(ya incluido arriba)

### manifests/namespace.yaml
(ya incluido arriba)

### manifests/user-deploy.yaml
(ya incluido arriba)

### manifests/user-service.yaml
(ya incluido arriba)

### manifests/content-deploy.yaml
(ya incluido arriba)

### manifests/content-service.yaml
(ya incluido arriba)

### infra/prometheus/alert-rules.yaml
(ya incluido arriba)

### infra/grafana/chapinflix-dashboard.json
(ya incluido arriba)

### infra/elk/elasticsearch-values.yaml
(ya incluido arriba)

### infra/elk/kibana-values.yaml
(ya incluido arriba)

### infra/elk/filebeat-values.yaml
(ya incluido arriba)

---

## Comandos √∫tiles resumen (copiar/pegar)

```bash
# 1. Obtener credenciales
gcloud container clusters get-credentials cluster-monitor --region us-central1

# 2. Crear namespaces
kubectl apply -f manifests/namespace.yaml

# 3. Desplegar microservicios
kubectl apply -f manifests/user-deploy.yaml
kubectl apply -f manifests/user-service.yaml
kubectl apply -f manifests/content-deploy.yaml
kubectl apply -f manifests/content-service.yaml

# 4. Prometheus + Grafana
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo add grafana https://grafana.github.io/helm-charts
helm repo update
kubectl create namespace monitoring
helm install prometheus prometheus-community/kube-prometheus-stack --namespace monitoring
helm install grafana grafana/grafana --namespace monitoring --set service.type=LoadBalancer --set adminUser=admin --set adminPassword=admin123

# 5. ELK + Filebeat
helm repo add elastic https://helm.elastic.co
helm repo update
kubectl create namespace elk
helm install elasticsearch elastic/elasticsearch --namespace elk -f infra/elk/elasticsearch-values.yaml
helm install kibana elastic/kibana --namespace elk -f infra/elk/kibana-values.yaml
helm install filebeat elastic/filebeat --namespace elk -f infra/elk/filebeat-values.yaml

# 6. Aplicar reglas Prometheus
kubectl create configmap prometheus-chapinflix-rules --from-file=infra/prometheus/alert-rules.yaml -n monitoring || kubectl replace configmap prometheus-chapinflix-rules --from-file=infra/prometheus/alert-rules.yaml -n monitoring
kubectl rollout restart deployment prometheus-kube-prometheus-stack-operator -n monitoring || true

# 7. Verificar
kubectl get pods -A
kubectl get svc -A
```

---

### Fin del documento.
